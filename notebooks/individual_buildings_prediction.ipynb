{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_config = {\n",
    "    'numerical': ['square_feet', 'year_built', 'floor_count', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', \n",
    "                 'sea_level_pressure', 'wind_speed', 'mean_target'],\n",
    "    'categorical': [\n",
    "#         'site_id', 'building_id', 'primary_use', \n",
    "        'wind_direction_cat', 'month', 'hour', 'season', 'daytime']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, d_in=10, k=2, n_hidden=1, batch_norm=False, dropout=False):\n",
    "        super(Net, self).__init__()\n",
    "        d_cur = d_in\n",
    "        self.layers = []\n",
    "        for i in range(n_hidden):\n",
    "            self.layers.append(nn.Linear(d_cur, d_cur // k))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(d_cur // k))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                self.layers.append(nn.Dropout())\n",
    "            d_cur //= k\n",
    "        self.layers.append(nn.Linear(d_cur, 1))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Prepare data\n",
    "\n",
    "def select_meter(df, meter=1):\n",
    "    df = df[df.meter == meter]\n",
    "    df.drop(columns='meter', inplace=True)\n",
    "    return df\n",
    "\n",
    "def filter_wind(weather_df):\n",
    "    weather_df.loc[weather_df.wind_direction + weather_df.wind_speed == 0, ['wind_direction', 'wind_speed']] = np.NaN\n",
    "    return weather_df\n",
    "\n",
    "def merge(data, weather, meta):\n",
    "    df = meta.merge(data, on='building_id')\n",
    "    df = df.merge(weather, on=['site_id', 'timestamp'])\n",
    "    return df\n",
    "\n",
    "def filter_zero_targets(df):\n",
    "    df = df[df.meter_reading != 0]\n",
    "    return df\n",
    "\n",
    "def create_new_features(df):\n",
    "    df['month'] = df.timestamp.apply(lambda x: time.strptime(x ,\"%Y-%m-%d %H:%M:%S\").tm_mon)\n",
    "    df['hour'] = df.timestamp.apply(lambda x: time.strptime(x ,\"%Y-%m-%d %H:%M:%S\").tm_hour)\n",
    "    \n",
    "    df['season'] = df['month'] % 12 // 3\n",
    "    df['daytime'] = df['hour'] // 5    \n",
    "    return df\n",
    "\n",
    "def prepare_data(meter=1, fast_debug=False):\n",
    "    meta = pd.read_csv('data/building_metadata.csv')\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    weather = pd.read_csv('data/weather_train.csv')\n",
    "    \n",
    "    train = select_meter(train, meter)\n",
    "    weather = filter_wind(weather)\n",
    "    df = merge(train, weather, meta)\n",
    "    \n",
    "    if fast_debug:\n",
    "        # building_ids = [1109, 1130, 1363, 1377]\n",
    "        building_ids = np.random.choice(df.building_id.unique(), 100, replace=False)\n",
    "        df = df[df.building_id.isin(building_ids)]\n",
    "\n",
    "    df = filter_zero_targets(df)\n",
    "    df = create_new_features(df)\n",
    "    return df\n",
    "\n",
    "def prepare_test_data(train_df, meter=1):\n",
    "    meta = pd.read_csv('data/building_metadata.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    weather = pd.read_csv('data/weather_test.csv')\n",
    "    \n",
    "    test = select_meter(test, meter)\n",
    "    weather = filter_wind(weather)\n",
    "    df = merge(test, weather, meta)\n",
    "    \n",
    "    building_ids = train_df.building_id.unique()\n",
    "    df = df[df.building_id.isin(building_ids)]\n",
    "\n",
    "    df['meter_reading'] = np.NaN\n",
    "    df = create_new_features(df)\n",
    "    return df\n",
    "\n",
    "# def filter_building(building_id):\n",
    "#     return df[df.building_id == building_id]\n",
    "    \n",
    "def save_results(submission, name):\n",
    "    submission.to_csv('results/%s' % name)\n",
    "    \n",
    "    \n",
    "# Preprocess data\n",
    "class Preprocessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        self.train_idx = self.train_test_split()\n",
    "        self.df = self.create_mean_target()\n",
    "        self.df = self.fill_nans()\n",
    "        self.df = self.create_wind_cat()\n",
    "        \n",
    "    def train_test_split(self):\n",
    "        if 'row_id' in self.df.columns:\n",
    "            train_idx = self.df[~pd.isna(self.df.meter_reading)].index\n",
    "            \n",
    "        else:\n",
    "            train_idx = np.random.choice(self.df.index, len(self.df) * 7 // 10, replace=False)\n",
    "        return train_idx\n",
    "        \n",
    "    def create_mean_target(self):\n",
    "        mean_targets = pd.DataFrame(data=self.df[self.df.index.isin(self.train_idx)].groupby('building_id').meter_reading.mean())\n",
    "        mean_targets.columns = ['mean_target']\n",
    "        \n",
    "        self.df['tmp_index'] = self.df.index\n",
    "        self.df.index = self.df.building_id\n",
    "        self.df['mean_target'] = mean_targets\n",
    "        self.df.index = self.df.tmp_index\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def fill_nans(self):\n",
    "        for col in ['square_feet', 'year_built', 'floor_count', 'air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                    'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']:\n",
    "            self.df[col] = self.df[col].fillna(self.df.loc[self.train_idx, col].mean())\n",
    "        return self.df\n",
    "    \n",
    "    def create_wind_cat(self):\n",
    "        self.df['wind_direction_cat'] = self.df['wind_direction'] // 45\n",
    "        return self.df\n",
    "\n",
    "# Scale data\n",
    "class Scaler:\n",
    "    def __init__(self, preprocessor, batch_size=512, prod=False):\n",
    "        self.df = preprocessor.df\n",
    "        self.train_idx = preprocessor.train_idx\n",
    "        \n",
    "        self.scaler_labels = None\n",
    "        self.scaler_features = None\n",
    "        self.encoders = {}\n",
    "        \n",
    "        self.create_scalers()\n",
    "        \n",
    "        cat_train, num_train, labels_train = self.transform(self.df.loc[self.train_idx])\n",
    "        cat_test, num_test, labels_test = self.transform(self.df[~self.df.index.isin(self.train_idx)])\n",
    "        self.d_in = cat_train.shape[1] + num_train.shape[1]\n",
    "        \n",
    "        if prod:\n",
    "            self.testloader = self.create_dataloader(cat_test, num_test, labels_test, batch_size * 100, shuffle=False, add_row_ids=True,\n",
    "                                             row_ids=self.df[~self.df.index.isin(self.train_idx)].row_id.values.reshape(-1, 1))\n",
    "        else:\n",
    "            self.testloader = self.create_dataloader(cat_test, num_test, labels_test, batch_size)\n",
    "        self.trainloader = self.create_dataloader(cat_train, num_train, labels_train, batch_size)\n",
    "  \n",
    "    def create_scalers(self):\n",
    "        self.scaler_features = StandardScaler()\n",
    "        self.scaler_labels = StandardScaler()\n",
    "\n",
    "        self.scaler_features.fit(self.df.loc[self.train_idx, columns_config['numerical']])\n",
    "        self.scaler_labels.fit(self.df.loc[self.train_idx, 'meter_reading'].values.reshape(-1, 1))\n",
    "        for col in columns_config['categorical']:\n",
    "            self.encoders[col] = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "            self.encoders[col].fit(self.df.loc[self.train_idx, col].values.reshape(-1, 1))\n",
    "        \n",
    "    def transform(self, data):\n",
    "        num_features = self.scaler_features.transform(data.loc[:, columns_config['numerical']])\n",
    "        labels = self.scaler_labels.transform(data.loc[:, 'meter_reading'].values.reshape(-1, 1))\n",
    "        cat_features = []\n",
    "        for col in columns_config['categorical']:\n",
    "            cat_features.append(self.encoders[col].transform(data[col].values.reshape(-1, 1)))\n",
    "        cat_features = np.concatenate(cat_features, axis=1)\n",
    "        return cat_features, num_features, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataloader(cat, num, labels, batch_size, shuffle=True, add_row_ids=False, row_ids=None):\n",
    "        data = [cat, num]\n",
    "        if add_row_ids:\n",
    "            data.append(row_ids)\n",
    "        dataset = TensorDataset(torch.Tensor(np.concatenate(data, 1)).to(device), torch.Tensor(labels).to(device))\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        return dataloader\n",
    "    \n",
    "# Train\n",
    "class Trainer:\n",
    "    def __init__(self, scaler, net_config, lr=0.001):\n",
    "        self.trainloader = scaler.trainloader\n",
    "        self.testloader = scaler.testloader\n",
    "        self.scaler_labels = scaler.scaler_labels\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "        self.net = None\n",
    "        \n",
    "        net_config['d_in'] = scaler.d_in\n",
    "        self.create_models(net_config, lr)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.metrics = []\n",
    "        \n",
    "    def create_models(self, net_config, lr):\n",
    "        self.net = Net(**net_config).to(device)\n",
    "        print('Net architecture:')\n",
    "        print(self.net)\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "    \n",
    "    def metric(self, pred, labels):\n",
    "        pred_raw = self.scaler_labels.inverse_transform(pred.detach().cpu().numpy())\n",
    "        labels_raw = self.scaler_labels.inverse_transform(labels.detach().cpu().numpy())\n",
    "        pred_raw[pred_raw < 0] = 0\n",
    "        loss =  np.mean((np.log(pred_raw + 1) - np.log(labels_raw + 1)) ** 2) ** 0.5\n",
    "        return loss\n",
    "    \n",
    "    def train(self, n_epochs, verbose=True, do_val=True):\n",
    "        for epoch in range(n_epochs):\n",
    "            self.net.train()\n",
    "            losses = []\n",
    "            for i, data in enumerate(self.trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.net(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "            if verbose:\n",
    "                print('[%d] Train loss: %.3f' % (epoch + 1, np.mean(losses)))\n",
    "            self.train_losses.append(np.mean(losses))\n",
    "\n",
    "            if do_val:\n",
    "                self.net.eval()\n",
    "                losses = []\n",
    "                metrics = []\n",
    "                for i, data in enumerate(self.testloader, 0):\n",
    "                    inputs, labels = data\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.net(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    losses.append(loss.item())\n",
    "                    metrics.append(self.metric(outputs, labels))\n",
    "                if verbose:\n",
    "                    print('[%d] Test loss: %.3f' % (epoch + 1, np.mean(losses)))\n",
    "                    print('[%d] Test metric: %.3f' % (epoch + 1, np.mean(metrics)))\n",
    "                self.test_losses.append(np.mean(losses))\n",
    "                self.metrics.append(np.mean(metrics))\n",
    "                \n",
    "    def predict(self, submission):\n",
    "        self.net.eval()\n",
    "        for data in self.testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, row_ids = inputs[:, :-1], inputs[:, -1]\n",
    "            print(row_ids)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.net(inputs)\n",
    "            pred_raw = self.scaler_labels.inverse_transform(outputs.detach().cpu().numpy())\n",
    "            pred_raw[pred_raw < 0] = 0\n",
    "            submission.loc[row_ids, 'meter_reading'] = pred_raw\n",
    "        return submission\n",
    "\n",
    "    def plot(self):\n",
    "        f, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10))\n",
    "        ax1.plot(self.train_losses, color='b')\n",
    "        ax1.plot(self.test_losses, color='y')\n",
    "        ax2.plot(self.metrics, color='y')\n",
    "        plt.show()\n",
    "        \n",
    "    def save_model(self, name):\n",
    "        torch.save(self.net, 'models/' + name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df = prepare_data(meter=0, fast_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_test_data(df, meter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['row_id'] = np.NaN\n",
    "df_all = pd.concat([df, test_df], axis=0)\n",
    "df_all = df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 103,   78,   53,   69,   30,   44,   51,   67,   72,   80,   45,\n",
       "        123,  192,  203,  208,  220,  231,  233,  237,  239,  265,  270,\n",
       "        289,  308,  318,  341,  355,  365,  369,  376,  379,  418,  430,\n",
       "        450,  467,  485,  488,  496,  536,  542,  546,  556,  300,  584,\n",
       "        587,  603,  642,  644,  669,  701,  711,  712,  727,  769,  776,\n",
       "        785,  803,  809,  810,  813,  823,  831,  832,  835,  842,  862,\n",
       "        865,  903,  928,  963,  965,  992,  892, 1006, 1014, 1019, 1039,\n",
       "       1043, 1047, 1056, 1065, 1070, 1098, 1100, 1106, 1124, 1125, 1146,\n",
       "       1230, 1231, 1254, 1260, 1266, 1279, 1325, 1348, 1378, 1381, 1389,\n",
       "       1440], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.building_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net architecture:\n",
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=21, bias=True)\n",
      "    (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=21, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[1] Train loss: 0.983\n",
      "[2] Train loss: 0.851\n",
      "[3] Train loss: 0.746\n",
      "[4] Train loss: 0.651\n",
      "[5] Train loss: 0.555\n",
      "[6] Train loss: 0.467\n",
      "[7] Train loss: 0.405\n",
      "[8] Train loss: 0.359\n",
      "[9] Train loss: 0.328\n",
      "[10] Train loss: 0.309\n",
      "tensor([1.2600e+02, 2.5500e+02, 3.8400e+02,  ..., 2.2598e+06, 2.2599e+06,\n",
      "        2.2601e+06])\n"
     ]
    }
   ],
   "source": [
    "df_b = df_all[df_all.building_id == 103]\n",
    "preprocessor = Preprocessor(df_b)\n",
    "scaler = Scaler(preprocessor, prod=True)\n",
    "trainer = Trainer(scaler, net_config, lr=0.001)\n",
    "trainer.train(10, verbose=True, do_val=False)\n",
    "submission = trainer.predict(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id           126.000000\n",
       "meter_reading     31.236124\n",
       "Name: 126, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "scaler = Scaler(preprocessor, prod=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net architecture:\n",
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=63, out_features=21, bias=True)\n",
      "    (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=21, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_config = {\n",
    "    'n_hidden': 1,\n",
    "    'batch_norm': True,\n",
    "    'dropout': True,\n",
    "    'k': 3\n",
    "}\n",
    "\n",
    "trainer = Trainer(scaler, net_config, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 0.161\n",
      "[2] Train loss: 0.112\n",
      "[3] Train loss: 0.109\n",
      "[4] Train loss: 0.108\n",
      "[5] Train loss: 0.106\n",
      "[6] Train loss: 0.107\n",
      "[7] Train loss: 0.107\n",
      "[8] Train loss: 0.108\n",
      "[9] Train loss: 0.106\n",
      "[10] Train loss: 0.105\n",
      "[11] Train loss: 0.105\n",
      "[12] Train loss: 0.105\n",
      "[13] Train loss: 0.107\n",
      "[14] Train loss: 0.106\n",
      "[15] Train loss: 0.106\n",
      "[16] Train loss: 0.103\n",
      "[17] Train loss: 0.104\n",
      "[18] Train loss: 0.104\n",
      "[19] Train loss: 0.107\n",
      "[20] Train loss: 0.105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-975d6c46fca8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-76-d48d952bc71f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, verbose, do_val)\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(150, verbose=True, do_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foggy\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "net_config = {\n",
    "    'n_hidden': 1,\n",
    "    'batch_norm': True,\n",
    "    'dropout': True,\n",
    "    'k': 3\n",
    "}\n",
    "    \n",
    "for building in buildings:\n",
    "    for meter in meters:\n",
    "        df = prepare_data(meter=meter, building_id=building)\n",
    "        preprocessor = Preprocessor(df)\n",
    "        scaler = Scaler(preprocessor)\n",
    "        trainer = Trainer(scaler, net_config, lr=0.001)\n",
    "        trainer.train(150, verbose=False)\n",
    "        print('Building %d, meter %d, last test loss %.4f, last test metric %.4f' % (building, meter, trainer.test_losses[-1],\n",
    "                                                                                    trainer.metrics[-1]))\n",
    "        trainer.save_model('model_%d_%d.pkl' % (building, meter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
